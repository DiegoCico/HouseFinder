# ğŸ¡ House Finder: Real Estate Data Scraper & Semantic Search Engine with AWS Integration

This project is an end-to-end real estate intelligence system that combines web scraping, semantic search, and AWS cloud architecture. It supports both **cloud-based workflows (S3, EC2, IAM)** and **local semantic search**, enabling complete data engineering and information retrieval pipelines.

---

## ğŸš€ Features

- ğŸ” **Property Scraper**: Scrapes real estate listings from Redfin.
- ğŸ˜ï¸ **Neighborhood Scraper**: Extracts neighborhood metadata including amenities, safety, and school ratings.
- â˜ï¸ **AWS Integration**: Uses **S3** for storage, **EC2** for scalable compute, and **IAM** for secure access control.
- ğŸ§  **Semantic Search**: Find neighborhoods matching natural language queries.
- ğŸ“ **Modular Architecture**: Split between `AWS/` for cloud-based processing and `SemanticSearchLocal/` for local querying.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ AWS/
â”‚   â”œâ”€â”€ Diagram.png                # Architecture diagram
â”‚   â”œâ”€â”€ exported_html.txt          # HTML snapshot
â”‚   â”œâ”€â”€ main.py                    # Main orchestrator for scraping
â”‚   â”œâ”€â”€ mainS3.py                  # Uploading scraped data to AWS S3
â”‚   â”œâ”€â”€ neighborhood_scraper.py    # Neighborhood info scraper
â”‚   â”œâ”€â”€ property_scraper.py        # Redfin property scraper
â”‚   â”œâ”€â”€ s3_cli.py                  # Command-line S3 uploader
â”‚   â”œâ”€â”€ s3_storage.py              # Upload/download helpers for S3
â”‚   â”œâ”€â”€ test.py / test.txt         # Sample test files
â”‚   â”œâ”€â”€ README.md / requirements.txt
â”œâ”€â”€ SemanticSearchLocal/
â”‚   â”œâ”€â”€ main.py                    # Runs local semantic search
â”‚   â”œâ”€â”€ neighborhood_scraper.py
â”‚   â”œâ”€â”€ property_scraper.py
â”‚   â”œâ”€â”€ semantic_search.py         # Embedding and querying logic
â”œâ”€â”€ url.txt                        # List of Redfin property URLs to track â€” system will auto-monitor them
â”œâ”€â”€ exported_data.csv              # Final output CSV combining property + neighborhood data
â”œâ”€â”€ .gitignore
```

---

## âš™ï¸ Backend Pipeline (Scraping + Cloud Upload)

Located in the `AWS/` directory:

- `property_scraper.py`: Crawls Redfin listings.
- `neighborhood_scraper.py`: Extracts contextual neighborhood data.
- `main.py`: Orchestrates scraping routines.
- `mainS3.py`: Handles JSON export and S3 uploading.
- `s3_storage.py`: Wraps AWS S3 upload/download logic.
- `s3_cli.py`: CLI to interact with AWS cloud.

**Infrastructure**: Use EC2 for deployment, IAM for access roles, and S3 for persistent storage.

---

## ğŸ§  Frontend Process (Local Semantic Search)

Located in the `SemanticSearchLocal/` directory:

- Uses `sentence-transformers` for embedding neighborhood descriptions.
- Stores vectors and matches user input using cosine similarity.
- Easily extendable to UI or web frontends.

### Example:
```python
from semantic_search import SemanticSearch
search = SemanticSearch("exported_data.json")
print(search.query("Quiet areas with good coffee and parks"))
```

---

## â˜ï¸ AWS Integration

- AWS credentials must be configured locally.
- JSON data can be pushed or pulled from S3 buckets.
- EC2 is recommended for long-running or scalable batch jobs.
- IAM roles should be configured to restrict S3 access securely.

---

## ğŸ› ï¸ Setup & Installation

### Clone the repo
```bash
git clone https://github.com/your-username/HouseFinder.git
cd HouseFinder
```

### Install dependencies
```bash
pip install -r requirements.txt
```

Typical dependencies:
```text
requests
beautifulsoup4
boto3
sentence-transformers
scikit-learn
```

---

## ğŸ”¢ Sample Usage

### Run full scraping pipeline:
```bash
python AWS/main.py
```

### Upload to S3:
```bash
python AWS/mainS3.py
```

### Run local semantic search:
To run semantic search locally, first execute the data scraping step with:
```bash
python SemanticSearchLocal/main.py
```
Then, launch the semantic search engine with:
```bash
python SemanticSearchLocal/semantic_search.py
```
```bash
python SemanticSearchLocal/main.py
```

---

## ğŸ“Š Example Output

The system exports all collected and enriched data into a structured CSV format for downstream use.

### CSV Sample Format:
```
Building Name,Address,Room Title,Bed/Baths,Price,Sqft,Availability,Cats Allowed,Dogs Allowed,Cat Rent,Dog Rent,Parking Type,Parking Fee,Assigned Parking,EV Parking Fee,Lease Term,Application fee,Livability,Amenities,Commute,Cost Of Living,Crime,Employment,Health,Housing,Schools,Ratings,amenities,commute,cost of living,crime,employment,health,housing,schools,ratings,Coffee,Entertainment,Food and Drink,Fitness,Groceries,Parks,Shops,Public Transit Stops,Workers Taking Public Transit,Cost of Living,Tax Rates,Property Crime,Violent Crime,Med. Household Income,Unemployment Rate,Health & Safety,Air Quality,Home Price,Home Appreciation Rate,Home Affordability,School Test Scores,High School Grad. Rates,Elementary Schools,High Schools,User Reviews,User Surveys
2nd and John,"200 2nd Ave W,Seattle, WA 98119",None,None,None,None,None,True,True,$40/mo,$50/mo,Garage Lot,$200/mo,Yes,$275 per Month,"2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 month",$14,68/100,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,A+,A+,F,F,A+,A+,B,A+,F,(26) A+,(24) A+,(36) A+,(7) A+,(5) A+,(12) A+,(27) A+,(38) A+,() A+,(N/A) F,() A+,(N/A) F,() F,(N/A) A+,() B-,(6) A+,(N/A) C-,(N/A) A+,(N/A) A+,(N/A) F,(N/A) B-,(N/A) A+,(1) B-,(1) B-,(0),(2) F
```

---

## ğŸ“œ License

MIT License

